{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create -n berlin python=3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  nuscenes-devkit fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring nuScenes in FiftyOne\n",
    "Due to the multi-sensor structure of nuScenes, the dataset in FiftyOne will be a [Grouped Dataset](https://docs.voxel51.com/user_guide/groups.html?_gl=1*1uqzezz*_gcl_au*MTI3OTEyMjEwMy4xNzI2MTQ5NDk3) with some [Dynamic Group Views](https://docs.voxel51.com/user_guide/using_views.html?_gl=1*1s195kz*_gcl_au*MTI3OTEyMjEwMy4xNzI2MTQ5NDk3#view-groups) thrown in there as well. At a high level, we will group together our samples by their associated scene in nuScenes. At regular intervals of each keyframe or approximately every 0.5 seconds (2Hz), we incorporate data from every sensor type, including their respective detections. This amalgamation of data results in distinct groups, each representing the sensor perspective at a given keyframe. We do have each sensor input for every frame, but since only keyframes are annotated, we choose to only load those in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting nuScenes\n",
    "To get started with nuScenes in FiftyOne, first we need to set up our environment for nuScenes. It will require downloading the dataset or a snippet of it as well as downloading the nuScenes python sdk. Full steps on installing can be found [here](https://www.nuscenes.org/nuscenes?tutorial=nuscenes).\n",
    "\n",
    "Once your nuScenes is installed into your computer, we can kick things off. Let’s start by initializing both nuScenes as well as our FiftyOne dataset. We define our dataset as well as add a group to initialize the dataset to expect grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes import NuScenes\n",
    "import fiftyone as fo\n",
    "\n",
    "dataroot='/path/to/drivestudio/data/nuscenes/raw/'\n",
    "nusc = NuScenes(version='v1.0-mini', dataroot=dataroot, verbose=True)\n",
    "\n",
    "dataset = fo.Dataset(\"nuscenes_cameras\",overwrite=True)\n",
    "dataset.add_group_field(\"group\", default=\"CAM_FRONT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Camera Data\n",
    "Camera data is a bit more straightforward than the lidar data. There is no need to do any prep on the image or save it as a different format. We can go right ahead and grab the detections to add them. The only tricky part is these are no ordinary bounding boxes, they are 3D bounding boxes given in global coordinates!\n",
    "\n",
    "Luckily for us, nuScenes provides some easy ways to convert their bounding boxes to our pixel space relative to what camera the image came from. For camera data, we load all of our boxes for our sample, check to see which ones are in the frame of our camera data, and then add the cuboids to the sample. In order to add a cuboid or a 3D bounding box, we use [polylines](https://docs.voxel51.com/user_guide/using_datasets.html?_gl=1*1vonm1t*_gcl_au*MTI3OTEyMjEwMy4xNzI2MTQ5NDk3#cuboids) and the [from_cuboid()](https://docs.voxel51.com/api/fiftyone.core.labels.html?_gl=1*z39nk3*_gcl_au*MTI3OTEyMjEwMy4xNzI2MTQ5NDk3#fiftyone.core.labels.Polyline.from_cuboid) method. Let’s take a look at how it is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.geometry_utils import box_in_image, view_points\n",
    "import numpy as np\n",
    "from nuscenes.utils.geometry_utils import  BoxVisibility\n",
    "from nuscenes.scripts.export_poses import derive_latlon\n",
    "\n",
    "def camera_sample(group, filepath, sensor, token, scene):\n",
    "    sample = fo.Sample(filepath=filepath, group=group.element(sensor))\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(token, box_vis_level=BoxVisibility.NONE,)\n",
    "    image = Image.open(data_path)\n",
    "    width, height = image.size\n",
    "    shape = (height,width)\n",
    "    polylines = []\n",
    "    log = nusc.get('log', scene[\"log_token\"])\n",
    "    location = log[\"location\"]\n",
    "    ego = nusc.get('ego_pose', data[\"ego_pose_token\"])\n",
    "    ego_list = [ego]\n",
    "\n",
    "    latlon = derive_latlon(location,ego_list)\n",
    "    lat = latlon[0][\"latitude\"]\n",
    "    lon = latlon[0][\"longitude\"]\n",
    "    sample[\"location\"] = fo.GeoLocation(\n",
    "        point = [lon,lat]\n",
    "    )\n",
    "    for box in boxes:\n",
    "        if box_in_image(box,camera_intrinsic,shape,vis_level=BoxVisibility.ALL):\n",
    "            c = np.array(nusc.colormap[box.name]) / 255.0\n",
    "            #print(box.name)\n",
    "            corners = view_points(box.corners(), camera_intrinsic, normalize=True)[:2, :]\n",
    "            front = [(corners[0][0]/width,corners[1][0]/height),\n",
    "                    (corners[0][1]/width,corners[1][1]/height),\n",
    "                    (corners[0][2]/width,corners[1][2]/height),\n",
    "                    (corners[0][3]/width,corners[1][3]/height),]\n",
    "            back =  [(corners[0][4]/width,corners[1][4]/height),\n",
    "                    (corners[0][5]/width,corners[1][5]/height),\n",
    "                    (corners[0][6]/width,corners[1][6]/height),\n",
    "                    (corners[0][7]/width,corners[1][7]/height),]\n",
    "            #print(corners.shape)\n",
    "            polylines.append(fo.Polyline.from_cuboid(front + back, label=box.name))\n",
    "    sample[\"cuboids\"] = fo.Polylines(polylines=polylines)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Samples to the Dataset\n",
    "Next we need to add our camera samples to make our first dataset! Lets loop through all the scenes and add each camera angle, forming groups of 6 samples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from nuscenes.utils.geometry_utils import view_points, BoxVisibility, box_in_image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define our sensor groups\n",
    "groups = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\",\n",
    "           \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\",]\n",
    "\n",
    "samples = []\n",
    "\n",
    "# Iterate over each scene\n",
    "for scene in nusc.scene:\n",
    "    my_scene = scene\n",
    "    token = my_scene['first_sample_token']\n",
    "    my_sample = nusc.get('sample', token)\n",
    "    last_sample_token = my_scene['last_sample_token']\n",
    "    \n",
    "    # Iterate over each sample in the scene\n",
    "    while not my_sample[\"next\"] == \"\":\n",
    "        scene_token = my_sample[\"scene_token\"]\n",
    "        group = fo.Group()\n",
    "        # Iterate over each sensor in the sample\n",
    "        for sensor in groups:\n",
    "            data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "            filepath = dataroot + data[\"filename\"]\n",
    "\n",
    "            # Check if the sensor is a camera\n",
    "            if data[\"sensor_modality\"] == \"camera\":\n",
    "                sample = camera_sample(group, filepath, sensor, my_sample['data'][sensor],scene)\n",
    "\n",
    "            # Add metadata to the sample\n",
    "            sample[\"token\"] = data[\"token\"]\n",
    "            sample[\"ego_pose_token\"] = data[\"ego_pose_token\"]\n",
    "            sample[\"calibrated_sensor_token\"] = data[\"calibrated_sensor_token\"]\n",
    "            sample[\"timestamp\"] = data[\"timestamp\"]\n",
    "            sample[\"is_key_frame\"] = data[\"is_key_frame\"]\n",
    "            sample[\"prev\"] = data[\"prev\"]\n",
    "            sample[\"next\"] = data[\"next\"]\n",
    "            sample[\"scene_token\"] = scene_token\n",
    "\n",
    "            \n",
    "            samples.append(sample)\n",
    "\n",
    "        token = my_sample[\"next\"]\n",
    "\n",
    "        my_sample = nusc.get('sample', token)\n",
    "\n",
    "# Add the samples to the dataset, group by scene_token, and launch the app     \n",
    "dataset.add_samples(samples)\n",
    "view = dataset.group_by(\"scene_token\", order_by=\"timestamp\")\n",
    "session = fo.launch_app(view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Point Cloud Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset for all sensors including LIDAR and RADAR\n",
    "all_sensor_dataset = fo.Dataset(\"nuscenes_sensors\",overwrite=True)\n",
    "all_sensor_dataset.add_group_field(\"group\", default=\"CAM_FRONT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a LIDAR sample from nuScenes is composed of two steps, generating the pointcloud and adding the detections. We must convert the binary point clouds to standard in order to ingest them. nuScenes also offers a LIDAR segmentation optional package that allows us to color each point cloud point a color corresponding to its class that we will be utilizing. We start with our lidar token, load in the color map and point cloud that corresponds to the token, and save them back to file with the new coloring and standard `pcd` point cloud file formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_io import load_bin_file\n",
    "from nuscenes.utils.color_map import get_colormap\n",
    "from nuscenes.lidarseg.lidarseg_utils import paint_points_label\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "import open3d as o3d\n",
    "import os\n",
    "\n",
    "def load_lidar(lidar_token):\n",
    "\n",
    "    #Grab and Generate Colormaps\n",
    "    gt_from = \"lidarseg\"\n",
    "    lidarseg_filename = dataroot + nusc.get(gt_from, lidar_token)['filename']\n",
    "    colormap = get_colormap()\n",
    "    name2index = nusc.lidarseg_name2idx_mapping\n",
    "\n",
    "    coloring = paint_points_label(lidarseg_filename,None,name2index, colormap=colormap)\n",
    "    filepath = dataroot + nusc.get(\"sample_data\", lidar_token)['filename']\n",
    "    root, extension = os.path.splitext(filepath)\n",
    "\n",
    "    #Load Point Cloud\n",
    "    cloud = LidarPointCloud.from_file(filepath)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(cloud.points[:3,:].T)\n",
    "    colors = coloring[:,:3]\n",
    "    colors.max()\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    #Save back Point Cloud\n",
    "    o3d.io.write_point_cloud(root, pcd)\n",
    "\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our point cloud file now properly prepared for ingestion, we can move along to adding detections. To do so, we grab all the detections from the keyframe. We use nuScenes SDK’s builtin box methods to retrieve the location, rotation, and dimensions of the box. To match FiftyOne’s [3D detection input](https://docs.voxel51.com/user_guide/using_datasets.html?_gl=1*mn36m*_gcl_au*MTI3OTEyMjEwMy4xNzI2MTQ5NDk3#d-detections), we take `box.orientation.yaw_pitch_roll` for rotation, `box.wlh` for width, length, and height, and  `box.center` for its location. Note too that `fo.Sample(filepath=filepath, group=group.element(sensor))` will automatically detect the pcd file and ingest the sample as a point cloud as well! After the method is run and detections are added, we have our LIDAR sample with detections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.geometry_utils import  BoxVisibility\n",
    "from nuscenes.scripts.export_poses import derive_latlon\n",
    "\n",
    "def lidar_sample(group, filepath, sensor, lidar_token, scene):\n",
    "    # Get the lidar data\n",
    "    data_path, boxes, camera_intrinsic = nusc.get_sample_data(lidar_token, box_vis_level=BoxVisibility.NONE,)\n",
    "    data = nusc.get('sample_data', lidar_token)\n",
    "    log = nusc.get('log', scene[\"log_token\"])\n",
    "    location = log[\"location\"]\n",
    "    ego = nusc.get('ego_pose', data[\"ego_pose_token\"])\n",
    "    ego_list = [ego]\n",
    "    latlon = derive_latlon(location,ego_list)\n",
    "    lat = latlon[0][\"latitude\"]\n",
    "    lon = latlon[0][\"longitude\"]\n",
    "\n",
    "    # Create a sample\n",
    "    sample = fo.Sample(filepath=filepath, group=group.element(sensor))\n",
    "\n",
    "    # Add the coords to the sample\n",
    "    sample[\"location\"] = fo.GeoLocation(\n",
    "        point = [lon,lat]\n",
    "    )\n",
    "    \n",
    "    # Add detections to the pcd\n",
    "    detections = []\n",
    "    for box in boxes:\n",
    "                    \n",
    "        x, y, z = box.orientation.yaw_pitch_roll\n",
    "        w, l, h = box.wlh.tolist()\n",
    "\n",
    "        detection = fo.Detection(\n",
    "                label=box.name,\n",
    "                location=box.center.tolist(),\n",
    "                rotation=[z, y, x],\n",
    "                dimensions=[l,w,h]\n",
    "                )\n",
    "        detections.append(detection)\n",
    "    sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RADAR is an interesting case. Since we have already stored our 3d detections in the LIDAR sample and RADAR is laid on top of the LIDAR in the 3D visualizer, we don’t need to copy our detections for each point cloud. The simplifies loading RADAR to just:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.utils.data_classes import RadarPointCloud\n",
    "from pyquaternion import Quaternion\n",
    "import numpy as np\n",
    "\n",
    "def load_radar(filepath, data ):\n",
    "\n",
    "    root, extension = os.path.splitext(filepath)\n",
    "    \n",
    "    #Load Point Cloud\n",
    "    pc = RadarPointCloud.from_file(filepath)\n",
    "\n",
    "    cs_record = nusc.get('calibrated_sensor', data['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    print(pc.points.shape)\n",
    "    pcd.points = o3d.utility.Vector3dVector(pc.points[:3,:].T)\n",
    "\n",
    "    #Save back Point Cloud\n",
    "    o3d.io.write_point_cloud(root+\"_NEW.pcd\", pcd)\n",
    "    \n",
    "    return root+\"_NEW.pcd\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it back again except this time we are taking cameras, LIDAR, and RADAR samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\",\n",
    "           \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\",\"LIDAR_TOP\", \"RADAR_FRONT\",\n",
    "           \"RADAR_FRONT_LEFT\", \"RADAR_FRONT_RIGHT\", \"RADAR_BACK_LEFT\", \"RADAR_BACK_RIGHT\"]\n",
    "\n",
    "samples = []\n",
    "# Iterate over each scene\n",
    "for scene in nusc.scene:\n",
    "    my_scene = scene\n",
    "    token = my_scene['first_sample_token']\n",
    "    my_sample = nusc.get('sample', token)\n",
    "    last_sample_token = my_scene['last_sample_token']\n",
    "    \n",
    "    # Iterate over each sample in the scene\n",
    "    while not my_sample[\"next\"] == \"\":\n",
    "        scene_token = my_sample[\"scene_token\"]\n",
    "        lidar_token = my_sample[\"data\"][\"LIDAR_TOP\"]\n",
    "        group = fo.Group()\n",
    "        # Iterate over each sensor in the sample\n",
    "        for sensor in groups:\n",
    "            data = nusc.get('sample_data', my_sample['data'][sensor])\n",
    "            filepath = dataroot + data[\"filename\"]\n",
    "\n",
    "            # Check if the sensor is lidar\n",
    "            if data[\"sensor_modality\"] == \"lidar\":\n",
    "                filepath = load_lidar(lidar_token)\n",
    "                sample = lidar_sample(group,filepath, sensor, lidar_token, scene)\n",
    "\n",
    "            # Check if the sensor is camera\n",
    "            elif data[\"sensor_modality\"] == \"camera\":\n",
    "                sample = camera_sample(group, filepath, sensor, my_sample['data'][sensor],scene)\n",
    "\n",
    "            # Else its radar\n",
    "            else:\n",
    "                radar_filepath = load_radar(filepath,data)\n",
    "                sample = fo.Sample(filepath=radar_filepath, group=group.element(sensor))\n",
    "\n",
    "            \n",
    "            # Add metadata to the sample\n",
    "            sample[\"token\"] = data[\"token\"]\n",
    "            sample[\"ego_pose_token\"] = data[\"ego_pose_token\"]\n",
    "            sample[\"calibrated_sensor_token\"] = data[\"calibrated_sensor_token\"]\n",
    "            sample[\"timestamp\"] = data[\"timestamp\"]\n",
    "            sample[\"is_key_frame\"] = data[\"is_key_frame\"]\n",
    "            sample[\"prev\"] = data[\"prev\"]\n",
    "            sample[\"next\"] = data[\"next\"]\n",
    "            sample[\"scene_token\"] = scene_token\n",
    "\n",
    "            \n",
    "            samples.append(sample)\n",
    "\n",
    "        token = my_sample[\"next\"]\n",
    "\n",
    "        my_sample = nusc.get('sample', token)\n",
    "\n",
    "# Add the samples to the dataset, group by scene_token, and launch the app\n",
    "all_sensor_dataset.add_samples(samples)\n",
    "view = all_sensor_dataset.group_by(\"scene_token\", order_by=\"timestamp\")\n",
    "session.dataset = all_sensor_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Techniques\n",
    "Let's begin looking at some advance methods we can do with our self driving dataset by flattening our group dataset. By default, a grouped dataset returns only one active slice when you grab a sample. If you want to look at all cameras at once, we can call `select_group_slices` and pass a media type or a list of group names to get the flattened samples back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below if you were unable to run the code above :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Run if you couldn't run above code\n",
    "all_sensor_dataset = foz.load_zoo_dataset(\"quickstart-groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the image samples from the dataset\n",
    "flattened_dataset = all_sensor_dataset.select_group_slices(media_type=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging Embeddings\n",
    "One of the strongest ways we can interact with our self driving car dataset is by using embeddings. By utilizing the [FiftyOne Brain](https://docs.voxel51.com/brain.html#), we can can compute embedddings on our dataset and do some awesome things. Let's check them out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Compute Visualization](https://docs.voxel51.com/brain.html#visualizing-embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    flattened_dataset,  \n",
    "    brain_key=\"embedding_viz\",\n",
    "    model=\"clip-vit-base32-torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Compute Similarity](https://docs.voxel51.com/brain.html#similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_similarity(\n",
    "    flattened_dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    brain_key=\"img_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = flattened_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the FiftyOne Brain to find :\n",
    "- [Most unique samples](https://docs.voxel51.com/brain.html#image-uniqueness)\n",
    "- [Likely label mistakes](https://docs.voxel51.com/brain.html#label-mistakes)\n",
    "- [Your hardest samples](https://docs.voxel51.com/brain.html#sample-hardness)\n",
    "- [Most Representative Samples](https://docs.voxel51.com/brain.html#image-representativeness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model Zoo\n",
    "\n",
    "We can use a ton of models right out of the box to get more insights into our dataset. Utilizing the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/index.html), we can load in some models to bring even more information to help with curation or downstream tasks. Let's try one of the most popular models right now in Meta's [SAM2](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "# Prompt with boxes\n",
    "flattened_dataset.apply_model(\n",
    "    model,\n",
    "    label_field=\"sam2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aftwerwards, you will be able to see some semantic segmentations of our image of the best things SAM2 decided to segment. You can also prompt SAM2 with your own bounding boxes for greater accuracy. If you aren't happy with the accuracy, don't worry, we will be using an even better model later!\n",
    "\n",
    "Let's try using a depth model to help us determine how far away other cars and objects are in our image samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\"depth-estimation-transformer-torch\")\n",
    "\n",
    "flattened_dataset.apply_model(model, label_field=\"depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our samples in our dataset now have a depth heatmap and a semantic sgementation on them. You can store as many different label types as you like on any sample in FIftyOne!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Techniques\n",
    "\n",
    "Next, we will be exploring the world of Gaussian Splats! Splats are a powerful way to model the world around you in a fast and accurate way. With splats, we can simulate car drives without having to use a real car on the street. But before we can do that, we need to generate the world in which we are running these simulations! We will be using [drivestudio](https://github.com/ziyc/drivestudio), an Open-Source project that allows you to leverage different self-driving car datasets and splat models to generate 3D Gaussian Splat worlds! Be sure to follow all the steps on their repo for install and setup. They also provide some data for each dataset including [NuScenes](https://github.com/ziyc/drivestudio/blob/main/docs/NuScenes.md) that is already preprocessed. We will be using that preprocesed data for our next step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processed cameras:\n",
    "#   idx    camera           original size\n",
    "#    0    CAM_FRONT         (900, 1600)\n",
    "#    1    CAM_FRONT_LEFT    (900, 1600)\n",
    "#    2    CAM_FRONT_RIGHT   (900, 1600)\n",
    "#    3    CAM_BACK_LEFT     (900, 1600)\n",
    "#    4    CAM_BACK_RIGHT    (900, 1600)\n",
    "#    5    CAM_BACK          (900, 1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map = {\"CAM_FRONT\": 0, \"CAM_FRONT_LEFT\": 1, \"CAM_FRONT_RIGHT\": 2, \"CAM_BACK_LEFT\": 3, \"CAM_BACK_RIGHT\": 4, \"CAM_BACK\": 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we define a quick mapping to our Group Names to their idx in the preprocessed data. Below, we will load in the first scene and begin to work with it as a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "dataset2 = dataset.match(F(\"scene_token\")==\"cc8c0bf57f984915a77078b10eb33198\").clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and make the dataset persistent\n",
    "dataset2.name=\"nuscenes_scene_0\"\n",
    "dataset.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is a lidar/image hz difference, so we need to adjust the frame number\n",
    "i = 0\n",
    "hz_adj = 5\n",
    "\n",
    "# Iterate over each group and add the masks\n",
    "cam_groups = [\"CAM_FRONT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_RIGHT\", \"CAM_BACK\",\n",
    "           \"CAM_BACK_LEFT\", \"CAM_FRONT_LEFT\",]\n",
    "for group in dataset2.iter_groups():\n",
    "    for sensor in group:\n",
    "        if sensor in cam_groups:\n",
    "            frame_num = i * hz_adj\n",
    "            cam_num = idx_map[sensor]\n",
    "            filename = f\"{str(frame_num).zfill(3)}_{cam_num}.png\"\n",
    "            sample = group[sensor]\n",
    "            sample[\"sky_mask\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/sky_masks/\"+filename\n",
    "                )\n",
    "            sample[\"dynamic_mask_all\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/dynamic_masks/all/\"+filename\n",
    "                )\n",
    "            sample[\"fine_dynamic_mask_all\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/fine_dynamic_masks/all/\"+filename\n",
    "                )\n",
    "            sample[\"dynamic_mask_human\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/dynamic_masks/human/\"+filename\n",
    "                )\n",
    "            sample[\"fine_dynamic_mask_human\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/fine_dynamic_masks/human/\"+filename\n",
    "                )\n",
    "            sample[\"dynamic_mask_vehicle\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/dynamic_masks/vehicle/\"+filename\n",
    "                )\n",
    "            sample[\"fine_dynamic_mask_vehicle\"] = fo.Segmentation(\n",
    "                mask_path=\"/path/to/drivestudio/data/nuscenes/processed_10Hz/mini/000/fine_dynamic_masks/vehicle/\"+filename\n",
    "                )\n",
    "            \n",
    "            sample.save()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the new dataset\n",
    "session.dataset = dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Splats\n",
    "After curating and loading in our preprocessed data, we can train the model on our dataset in `drivestudio`. Make sure you have a powerful GPU to do so! You might need 12GB+ of RAM! After the model trains, we can view our new gaussian splats as videos as well as export them into our [FiftyOne KSplat Viewer](https://github.com/danielgural/ksplats_panel)! To get the files, look [here](https://drive.google.com/file/d/1JuaUFSxF-7aRBI4jWST2WwmTLHgC8-PQ/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Create a new dataset and load our splats in\n",
    "dataset3 = fo.Dataset(\"nuscenes_splats\",overwrite=True)\n",
    "dataset3.add_group_field(\"group\", default=\"Background_RGBS\")\n",
    "group = fo.Group()\n",
    "samples = []\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_Background_rgbs.mp4\",\n",
    "     group=group.element(\"Background_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_DeformableNodes_rgbs.mp4\",\n",
    "     group=group.element(\"DeformableNodes_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_Dynamic_rgbs.mp4\",\n",
    "     group=group.element(\"Dynamic_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_gt_rgbs.mp4\",\n",
    "     group=group.element(\"GT_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_rgbs.mp4\",\n",
    "     group=group.element(\"RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_RigidNodes_rgbs.mp4\",\n",
    "     group=group.element(\"RigidNodes_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "sample = fo.Sample(\n",
    "    filepath=\"/path/to/videos/full_set_30000_SMPLNodes_rgbs.mp4\",\n",
    "     group=group.element(\"SMPLNodes_RGBS\")\n",
    "     )\n",
    "samples.append(sample)\n",
    "\n",
    "dataset3.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.splats import SplatFile\n",
    "\n",
    "# Define the splat file on our sample as well!\n",
    "sample = dataset3.first()\n",
    "sample[\"splat\"] = SplatFile(filepath=\"/path/to/background.ksplat\")\n",
    "sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
